{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLMypoj6-HVb"
   },
   "source": [
    "\n",
    "**Deep Learning From Scratch - Theory and Implementation**\n",
    "https://www.codingame.com/playgrounds/9487/deep-learning-from-scratch---theory-and-implementation/computational-graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rB2Hd_i-gss"
   },
   "source": [
    "**Operations**\n",
    "\n",
    "Every operation is characterized by three things:\n",
    "\n",
    "* A compute function that computes the operation's output given values for the operation's inputs\n",
    "* A list of input_nodes which can be variables or other operations\n",
    "* A list of consumers that use the operation's output as their input\n",
    "\n",
    "Let's put this into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFgD0nmK-Yqe"
   },
   "outputs": [],
   "source": [
    "class Operation:\n",
    "    \"\"\"Represents a graph node that performs a computation.\n",
    "\n",
    "    An `Operation` is a node in a `Graph` that takes zero or\n",
    "    more objects as input, and produces zero or more objects\n",
    "    as output.\n",
    "    \"\"\"\n",
    "    # constructor\n",
    "    def __init__(self, input_nodes=[]):\n",
    "        \"\"\"Construct Operation\n",
    "        \"\"\"\n",
    "        self.input_nodes = input_nodes\n",
    "\n",
    "        # Initialize list of consumers (i.e. nodes that receive this operation's output as input)\n",
    "        self.consumers = []\n",
    "\n",
    "        # Append this operation to the list of consumers of all input nodes\n",
    "        for input_node in input_nodes:\n",
    "            input_node.consumers.append(self)\n",
    "\n",
    "        # Append this operation to the list of operations in the currently active default graph\n",
    "        _default_graph.operations.append(self)\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"Computes the output of this operation.\n",
    "        \"\" Must be implemented by the particular operation.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L02FdiLw-qxh"
   },
   "source": [
    "**Some elementary operations**\n",
    "\n",
    "Let's implement some elementary operations in order to become familiar with the Operation class (and because we will need them later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UK40wso4-cN-"
   },
   "outputs": [],
   "source": [
    "class add(Operation):\n",
    "    \"\"\"Returns x + y element-wise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"Construct add\n",
    "\n",
    "        Args:\n",
    "          x: First summand node\n",
    "          y: Second summand node\n",
    "        \"\"\"\n",
    "        super().__init__([x, y])\n",
    "\n",
    "    def compute(self, x_value, y_value):\n",
    "        \"\"\"Compute the output of the add operation\n",
    "\n",
    "        Args:\n",
    "          x_value: First summand value\n",
    "          y_value: Second summand value\n",
    "        \"\"\"\n",
    "        return x_value + y_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgTJYjOe-zDB"
   },
   "outputs": [],
   "source": [
    "class matmul(Operation):\n",
    "    \"\"\"Multiplies matrix a by matrix b, producing a * b.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a, b):\n",
    "        \"\"\"Construct matmul\n",
    "\n",
    "        Args:\n",
    "          a: First matrix\n",
    "          b: Second matrix\n",
    "        \"\"\"\n",
    "        super().__init__([a, b])\n",
    "\n",
    "    def compute(self, a_value, b_value):\n",
    "        \"\"\"Compute the output of the matmul operation\n",
    "\n",
    "        Args:\n",
    "          a_value: First matrix value\n",
    "          b_value: Second matrix value\n",
    "        \"\"\"\n",
    "        return a_value.dot(b_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIyhmDJB5UlU"
   },
   "outputs": [],
   "source": [
    "class log(Operation):\n",
    "    \"\"\"Computes the natural logarithm of x element-wise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x):\n",
    "        \"\"\"Construct log\n",
    "\n",
    "        Args:\n",
    "          x: Input node\n",
    "        \"\"\"\n",
    "        super().__init__([x])\n",
    "\n",
    "    def compute(self, x_value):\n",
    "        \"\"\"Compute the output of the log operation\n",
    "\n",
    "        Args:\n",
    "          x_value: Input value\n",
    "        \"\"\"\n",
    "        return np.log(x_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEcxUuDN5VdY"
   },
   "outputs": [],
   "source": [
    "class multiply(Operation):\n",
    "    \"\"\"Returns x * y element-wise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"Construct multiply\n",
    "\n",
    "        Args:\n",
    "          x: First multiplicand node\n",
    "          y: Second multiplicand node\n",
    "        \"\"\"\n",
    "        super().__init__([x, y])\n",
    "\n",
    "    def compute(self, x_value, y_value):\n",
    "        \"\"\"Compute the output of the multiply operation\n",
    "\n",
    "        Args:\n",
    "          x_value: First multiplicand value\n",
    "          y_value: Second multiplicand value\n",
    "        \"\"\"\n",
    "        return x_value * y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4h_JAuB75l6P"
   },
   "outputs": [],
   "source": [
    "class negative(Operation):\n",
    "    \"\"\"Computes the negative of x element-wise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x):\n",
    "        \"\"\"Construct negative\n",
    "\n",
    "        Args:\n",
    "          x: Input node\n",
    "        \"\"\"\n",
    "        super().__init__([x])\n",
    "\n",
    "    def compute(self, x_value):\n",
    "        \"\"\"Compute the output of the negative operation\n",
    "\n",
    "        Args:\n",
    "          x_value: Input value\n",
    "        \"\"\"\n",
    "        return -x_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oFC8RBI-44A"
   },
   "source": [
    "**Placeholders**\n",
    "\n",
    "Not all the nodes in a computational graph are operations. For example, in the affine transformation graph, ,  and  are not operations. Rather, they are inputs to the graph that have to be supplied with a value once we want to compute the output of the graph. To provide such values, we introduce placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2s9zACBP-1y7"
   },
   "outputs": [],
   "source": [
    "class placeholder:\n",
    "    \"\"\"Represents a placeholder node that has to be provided with a value\n",
    "       when computing the output of a computational graph\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Construct placeholder\n",
    "        \"\"\"\n",
    "        self.consumers = []\n",
    "\n",
    "        # Append this placeholder to the list of placeholders in the currently active default graph\n",
    "        _default_graph.placeholders.append(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY2NVFxk-9a9"
   },
   "source": [
    "**Variables**\n",
    "\n",
    "In the affine transformation graph, there is a qualitative difference between  on the one hand and  and  on the other hand. While  is an input to the operation, A and b are parameters of the operation, i.e. they are intrinsic to the graph. We will refer to such parameters as Variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZb3GqYI_BmQ"
   },
   "outputs": [],
   "source": [
    "class Variable:\n",
    "    \"\"\"Represents a variable (i.e. an intrinsic, changeable parameter of a computational graph).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_value=None):\n",
    "        \"\"\"Construct Variable\n",
    "\n",
    "        Args:\n",
    "          initial_value: The initial value of this variable\n",
    "        \"\"\"\n",
    "        self.value = initial_value\n",
    "        self.consumers = []\n",
    "\n",
    "        # Append this variable to the list of variables in the currently active default graph\n",
    "        _default_graph.variables.append(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgPSqdap_Spk"
   },
   "source": [
    "**The Graph class**\n",
    "\n",
    "Finally, we'll need a class that bundles all the operations, placeholders and variables together. When creating a new graph, we can call its as_default method to set the _default_graph to this graph. This way, we can create operations, placeholders and variables without having to pass in a reference to the graph everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzSXJ07l_TH2"
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \"\"\"Represents a computational graph\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Construct Graph\"\"\"\n",
    "        self.operations = []\n",
    "        self.placeholders = []\n",
    "        self.variables = []\n",
    "\n",
    "    def as_default(self):\n",
    "        global _default_graph\n",
    "        _default_graph = self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0p7MSR7exmGw"
   },
   "source": [
    "**Session: Computing the output of an operation**\n",
    "\n",
    "Now that we are confident creating computational graphs, we can start to think about how to compute the output of an operation.\n",
    "\n",
    "Let's create a Session class that encapsulates an execution of an operation. We would like to be able to create a **session** instance and call a run method on this instance, passing the operation that we want to compute and a dictionary containing values for the placeholders:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "z = \\begin{bmatrix}\n",
    "1&0\\\\0&-1\n",
    "\\end{bmatrix}.\\begin{bmatrix}\n",
    "1\\\\2\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1\\\\1\n",
    "\\end{bmatrix}.[2] = \\begin{bmatrix}\n",
    "2\\\\-1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "In order to compute the function represented by an operation, we need to apply the computations in the right order. For example, we cannot compute $z$ before we have computed $y$ as an intermediate result. Therefore, we have to make sure that the operations are carried out in the right order, such that the values of every node that is an input to an operation $0$ has been computed before $0$ is computed. This can be achieved via post-order traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUY97rLSxlcF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Session:\n",
    "    \"\"\"Represents a particular execution of a computational graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, operation, feed_dict={}):\n",
    "        \"\"\"Computes the output of an operation\n",
    "\n",
    "        Args:\n",
    "          operation: The operation whose output we'd like to compute.\n",
    "          feed_dict: A dictionary that maps placeholders to values for this session\n",
    "        \"\"\"\n",
    "\n",
    "        # Perform a post-order traversal of the graph to bring the nodes into the right order\n",
    "        nodes_postorder = traverse_postorder(operation)\n",
    "        print(\"Forwrad: Nodes Postorder\", nodes_postorder)\n",
    "        # Iterate all nodes to determine their value\n",
    "        for node in nodes_postorder:\n",
    "\n",
    "            if type(node) == placeholder:\n",
    "                # Set the node value to the placeholder value from feed_dict\n",
    "                node.output = feed_dict[node]\n",
    "            elif type(node) == Variable:\n",
    "                # Set the node value to the variable's value attribute\n",
    "                node.output = node.value\n",
    "            else:  # Operation\n",
    "                # Get the input values for this operation from the output values of the input nodes\n",
    "                node.inputs = [input_node.output for input_node in node.input_nodes]\n",
    "                print(\"Node Inputs\", node.inputs)\n",
    "\n",
    "                # Compute the output of this operation\n",
    "                node.output = node.compute(*node.inputs)\n",
    "\n",
    "            # Convert lists to numpy arrays\n",
    "            if type(node.output) == list:\n",
    "                node.output = np.array(node.output)\n",
    "\n",
    "        # Return the requested node value\n",
    "        return operation.output\n",
    "\n",
    "\n",
    "def traverse_postorder(operation):\n",
    "    \"\"\"Performs a post-order traversal, returning a list of nodes\n",
    "    in the order in which they have to be computed\n",
    "\n",
    "    Args:\n",
    "       operation: The operation to start traversal at\n",
    "    \"\"\"\n",
    "\n",
    "    nodes_postorder = []\n",
    "\n",
    "    def recurse(node):\n",
    "        if isinstance(node, Operation):\n",
    "            for input_node in node.input_nodes:\n",
    "                recurse(input_node)\n",
    "        nodes_postorder.append(node)\n",
    "\n",
    "    recurse(operation)\n",
    "    return nodes_postorder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djwevo8d_bLm"
   },
   "source": [
    "Let's now use the classes we have built to create a computational graph for the following affine transformation:\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "1&0&1\\\\\n",
    "0&-1&-1\\\\\n",
    "1&2&3\n",
    "\\end{bmatrix}.x + \\begin{bmatrix}\n",
    "1\\\\1\\\\1\n",
    "\\end{bmatrix}.[m]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruQjTkN8_aqI"
   },
   "outputs": [],
   "source": [
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "# Create variables\n",
    "A = Variable([[1, 0,1], [0, -1,2],[0, -1,2]])\n",
    "b = Variable([[1], [1],[1]])\n",
    "\n",
    "\n",
    "# Create placeholder\n",
    "x = placeholder()\n",
    "\n",
    "# Create placeholder\n",
    "m = placeholder()\n",
    "\n",
    "# Create hidden node y\n",
    "y = matmul(A, x)\n",
    "\n",
    "# Create hidden node n\n",
    "n = matmul(b,m)\n",
    "\n",
    "# Create output node z\n",
    "z = add(y, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJF2_vdWyMtn",
    "outputId": "57e92b6e-9831-4852-9964-9221007886b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forwrad: Nodes Postorder [<__main__.Variable object at 0x7f963548dad0>, <__main__.placeholder object at 0x7f963548dd50>, <__main__.matmul object at 0x7f963548d290>, <__main__.Variable object at 0x7f963548ded0>, <__main__.placeholder object at 0x7f963548de10>, <__main__.matmul object at 0x7f963548d410>, <__main__.add object at 0x7f963548d510>]\n",
      "Node Inputs [array([[ 1,  0,  1],\n",
      "       [ 0, -1,  2],\n",
      "       [ 0, -1,  2]]), array([1, 1, 1])]\n",
      "Node Inputs [array([[1],\n",
      "       [1],\n",
      "       [1]]), array([3])]\n",
      "Node Inputs [array([2, 1, 1]), array([3, 3, 3])]\n",
      "Output:  [5 4 4]\n"
     ]
    }
   ],
   "source": [
    "session = Session()\n",
    "output = session.run(z, {\n",
    "    x: [1,1,1], m: [3]\n",
    "})\n",
    "\n",
    "print(\"Output: \",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyQSZbu4plgT"
   },
   "source": [
    "Let's now use the classes we have built to create a computational graph for the following affine transformation:\n",
    "\\begin{equation}\n",
    "g = (x+y)*z\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBmTC8oqpmIj"
   },
   "outputs": [],
   "source": [
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "# Create placeholder\n",
    "x = placeholder()\n",
    "y = placeholder()\n",
    "z = placeholder()\n",
    "\n",
    "# Create hidden node p\n",
    "p = add(x, y)\n",
    "\n",
    "# Create output node g\n",
    "g = multiply(p,z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJ4J910SAf3s",
    "outputId": "459b3d43-55dc-41ba-f8e8-9f0f308c9e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forwrad: Nodes Postorder [<__main__.placeholder object at 0x7f96354a2610>, <__main__.placeholder object at 0x7f96354a2390>, <__main__.add object at 0x7f96354a2350>, <__main__.placeholder object at 0x7f96354a2490>, <__main__.multiply object at 0x7f96354a2250>]\n",
      "Node Inputs [array([1]), array([3])]\n",
      "Node Inputs [array([4]), array([-3])]\n",
      "Output:  [-12]\n"
     ]
    }
   ],
   "source": [
    "session = Session()\n",
    "output = session.run(g, {\n",
    "    x: [1], y: [3], z: [-3]\n",
    "})\n",
    "\n",
    "print(\"Output: \",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQFsHVWKB_k8"
   },
   "source": [
    "**Backward Pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSb3n3BbB3bv"
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "class Backward:\n",
    "    #def __init__(self):\n",
    "        # self.learning_rate = learning_rate\n",
    "\n",
    "    def backward(self, loss):\n",
    "        # learning_rate = self.learning_rate\n",
    "\n",
    "        class BackwardOperation(Operation):\n",
    "            def compute(self):\n",
    "                # Compute gradients\n",
    "                grad_table = compute_gradients(loss)\n",
    "\n",
    "                # Iterate all variables\n",
    "                for node in grad_table:\n",
    "                    if type(node) == Variable:\n",
    "                        # Retrieve gradient for this variable\n",
    "                        grad = grad_table[node]\n",
    "\n",
    "                        # Take a step along the direction of the negative gradient\n",
    "                        node.value = grad\n",
    "\n",
    "        return BackwardOperation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUA6ZN-Eug7Z"
   },
   "source": [
    "**Backpropagation**\n",
    "As a prerequisite to implementing backpropagation, we need to specify a function for each operation that computes the gradients with respect to the inputs of that operation, given the gradients with respect to the output. Let's define a decorator @RegisterGradient(operation_name) for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdjQkYURuicg"
   },
   "outputs": [],
   "source": [
    "# A dictionary that will map operations to gradient functions\n",
    "_gradient_registry = {}\n",
    "\n",
    "\n",
    "class RegisterGradient:\n",
    "    \"\"\"A decorator for registering the gradient function for an op type.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, op_type):\n",
    "        \"\"\"Creates a new decorator with `op_type` as the Operation type.\n",
    "        Args:\n",
    "          op_type: The name of an operation\n",
    "        \"\"\"\n",
    "        self._op_type = eval(op_type)\n",
    "\n",
    "    def __call__(self, f):\n",
    "        \"\"\"Registers the function `f` as gradient function for `op_type`.\"\"\"\n",
    "        _gradient_registry[self._op_type] = f\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1LKLefwuutY"
   },
   "source": [
    "Now assume that our _gradient_registry dictionary is already filled with gradient computation functions for all of our operations. We can now implement backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNIc8cN9uvSK"
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "\n",
    "def compute_gradients(loss):\n",
    "    print(\"compute_gradients\")\n",
    "    # grad_table[node] will contain the gradient of the loss w.r.t. the node's output\n",
    "    grad_table = {}\n",
    "\n",
    "    # The gradient of the loss with respect to the loss is just 1\n",
    "    grad_table[loss] = 1\n",
    "\n",
    "    # Perform a breadth-first search, backwards from the loss\n",
    "    visited = set()\n",
    "    queue = Queue()\n",
    "    visited.add(loss)\n",
    "    queue.put(loss)\n",
    "\n",
    "    while not queue.empty():\n",
    "        node = queue.get()\n",
    "\n",
    "        # If this node is not the loss\n",
    "        if node != loss:\n",
    "            #\n",
    "            # Compute the gradient of the loss with respect to this node's output\n",
    "            #\n",
    "            grad_table[node] = 0\n",
    "\n",
    "            # Iterate all consumers\n",
    "            for consumer in node.consumers:\n",
    "                print(\"\\nConsumer: \", consumer)\n",
    "                # Retrieve the gradient of the loss w.r.t. consumer's output\n",
    "                lossgrad_wrt_consumer_output = grad_table[consumer]\n",
    "                print(\"grad wrt output: \", lossgrad_wrt_consumer_output)\n",
    "                # Retrieve the function which computes gradients with respect to\n",
    "                # consumer's inputs given gradients with respect to consumer's output.\n",
    "                consumer_op_type = consumer.__class__\n",
    "                bprop = _gradient_registry[consumer_op_type]\n",
    "\n",
    "                # Get the gradient of the loss with respect to all of consumer's inputs\n",
    "                lossgrads_wrt_consumer_inputs = bprop(consumer, lossgrad_wrt_consumer_output)\n",
    "                print(\"grad wrt inputs: \",lossgrads_wrt_consumer_inputs)\n",
    "\n",
    "                if len(consumer.input_nodes) == 1:\n",
    "                    # If there is a single input node to the consumer, lossgrads_wrt_consumer_inputs is a scalar\n",
    "                    grad_table[node] += lossgrads_wrt_consumer_inputs\n",
    "\n",
    "                else:\n",
    "                    # Otherwise, lossgrads_wrt_consumer_inputs is an array of gradients for each input node\n",
    "\n",
    "                    # Retrieve the index of node in consumer's inputs\n",
    "                    node_index_in_consumer_inputs = consumer.input_nodes.index(node)\n",
    "\n",
    "                    # Get the gradient of the loss with respect to node\n",
    "                    lossgrad_wrt_node = lossgrads_wrt_consumer_inputs[node_index_in_consumer_inputs]\n",
    "\n",
    "                    # Add to total gradient\n",
    "                    grad_table[node] += lossgrad_wrt_node\n",
    "\n",
    "                print(\"Grad Node: \", grad_table[node])\n",
    "\n",
    "        #\n",
    "        # Append each input node to the queue\n",
    "        #\n",
    "        if hasattr(node, \"input_nodes\"):\n",
    "            for input_node in node.input_nodes:\n",
    "                if not input_node in visited:\n",
    "                    visited.add(input_node)\n",
    "                    queue.put(input_node)\n",
    "\n",
    "    # Return gradients for each visited node\n",
    "    return grad_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NkJxeUZux33"
   },
   "source": [
    "**Gradient of each operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z4-_20hu43Y"
   },
   "source": [
    "Gradient for negative\n",
    "Given a gradient $G$ with respect to $-x$, the gradient with respect to $x$ is given by $-G$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItWA1i6Mu3kI"
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"negative\")\n",
    "def _negative_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `negative`.\n",
    "\n",
    "    Args:\n",
    "      op: The `negative` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `negative` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `negative`.\n",
    "    \"\"\"\n",
    "    return -grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGEwBGpJvE4i"
   },
   "source": [
    "**Gradient for log**\n",
    "Given a gradient $G$ with respect to $log(x)$, the gradient with respect to $x$ is given by $\\frac{G}{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Vo1SY6kvVU7"
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"log\")\n",
    "def _log_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `log`.\n",
    "\n",
    "    Args:\n",
    "      op: The `log` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `log` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `log`.\n",
    "    \"\"\"\n",
    "    x = op.inputs[0]\n",
    "    return grad/x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn7-82hQvXD3"
   },
   "source": [
    "**Gradient for multiply**\n",
    "Given a gradient $G$ with respect to $A\\odot B$, the gradient with respect to $A$ is given by $G\\odot B$ and the gradient with respect to $B$ is given by $G\\odot A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgu6-FkVv2dQ"
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"multiply\")\n",
    "def _multiply_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `multiply`.\n",
    "\n",
    "    Args:\n",
    "      op: The `multiply` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `multiply` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `multiply`.\n",
    "    \"\"\"\n",
    "\n",
    "    A = op.inputs[0]\n",
    "    B = op.inputs[1]\n",
    "\n",
    "    return [grad * B, grad * A]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDecFzR_v2_z"
   },
   "source": [
    "**Gradient for matmul**\n",
    "Given a gradient G with respect to $AB$, the gradient with respect to $A$ is given by $GB^T$ and the gradient with respect to $B$ is given by $A^TG$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxdpQllpwJUw"
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"matmul\")\n",
    "def _matmul_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `matmul`.\n",
    "\n",
    "    Args:\n",
    "      op: The `matmul` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `matmul` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `matmul`.\n",
    "    \"\"\"\n",
    "\n",
    "    A = op.inputs[0]\n",
    "    B = op.inputs[1]\n",
    "\n",
    "    return [grad.dot(B.T), A.T.dot(grad)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7284bkHwK3q"
   },
   "source": [
    "**Gradient for add**\n",
    "Given a gradient G with respect to $a+b$, the gradient $G$ with respect to $a$ is given by $G$ and the gradient with respect to $b$ is also given by $G$, provided that $a$ and $b$ are of the same shape. If $a$ and $b$ are of different shapes, e.g. one matrix  with 100 rows and one row vector $b$, we assume that $b$ is added to each row of $a$. In this case, the gradient computation is a little more involved, but I will not spell out the details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdj46dpuwtW3"
   },
   "outputs": [],
   "source": [
    "@RegisterGradient(\"add\")\n",
    "def _add_gradient(op, grad):\n",
    "    \"\"\"Computes the gradients for `add`.\n",
    "\n",
    "    Args:\n",
    "      op: The `add` `Operation` that we are differentiating\n",
    "      grad: Gradient with respect to the output of the `add` op.\n",
    "\n",
    "    Returns:\n",
    "      Gradients with respect to the input of `add`.\n",
    "    \"\"\"\n",
    "    a = op.inputs[0]\n",
    "    b = op.inputs[1]\n",
    "\n",
    "    grad_wrt_a = grad\n",
    "    grad_wrt_b = grad\n",
    "\n",
    "    #\n",
    "    # The following becomes relevant if a and b are of different shapes.\n",
    "    #\n",
    "    while np.ndim(grad_wrt_a) > len(a.shape):\n",
    "        grad_wrt_a = np.sum(grad_wrt_a, axis=0)\n",
    "    for axis, size in enumerate(a.shape):\n",
    "        if size == 1:\n",
    "            grad_wrt_a = np.sum(grad_wrt_a, axis=axis, keepdims=True)\n",
    "\n",
    "    while np.ndim(grad_wrt_b) > len(b.shape):\n",
    "        grad_wrt_b = np.sum(grad_wrt_b, axis=0)\n",
    "    for axis, size in enumerate(b.shape):\n",
    "        if size == 1:\n",
    "            grad_wrt_b = np.sum(grad_wrt_b, axis=axis, keepdims=True)\n",
    "\n",
    "    return [grad_wrt_a, grad_wrt_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDgYqVP3wy_g"
   },
   "source": [
    "**Example**\n",
    "Let's now test our implementation to compute the backward pass of computation graph for the following affine transformation:\n",
    "\\begin{equation}\n",
    "g = (x+y)*z\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AHUz6MSwz9-",
    "outputId": "e01c61b7-a933-472d-938d-94c8dc666b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forwrad: Nodes Postorder [<__main__.placeholder object at 0x7f963548d8d0>, <__main__.placeholder object at 0x7f9634253750>, <__main__.add object at 0x7f9634253e50>, <__main__.placeholder object at 0x7f9634253f10>, <__main__.multiply object at 0x7f9634253490>]\n",
      "Node Inputs [array([1]), array([3])]\n",
      "Node Inputs [array([4]), array([-3])]\n",
      "Forward Output:  [-12]\n",
      "Forwrad: Nodes Postorder [<__main__.Backward.backward.<locals>.BackwardOperation object at 0x7f963423ee90>]\n",
      "Node Inputs []\n",
      "compute_gradients\n",
      "\n",
      "Consumer:  <__main__.multiply object at 0x7f9634253490>\n",
      "grad wrt output:  1\n",
      "grad wrt inputs:  [array([-3]), array([4])]\n",
      "Grad Node:  [-3]\n",
      "\n",
      "Consumer:  <__main__.multiply object at 0x7f9634253490>\n",
      "grad wrt output:  1\n",
      "grad wrt inputs:  [array([-3]), array([4])]\n",
      "Grad Node:  [4]\n",
      "\n",
      "Consumer:  <__main__.add object at 0x7f9634253e50>\n",
      "grad wrt output:  [-3]\n",
      "grad wrt inputs:  [array([-3]), array([-3])]\n",
      "Grad Node:  [-3]\n",
      "\n",
      "Consumer:  <__main__.add object at 0x7f9634253e50>\n",
      "grad wrt output:  [-3]\n",
      "grad wrt inputs:  [array([-3]), array([-3])]\n",
      "Grad Node:  [-3]\n",
      "Backward Output:  None\n"
     ]
    }
   ],
   "source": [
    "# Create a new graph\n",
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "# Create placeholder\n",
    "x = placeholder()\n",
    "y = placeholder()\n",
    "z = placeholder()\n",
    "\n",
    "# Create hidden node p\n",
    "p = add(x, y)\n",
    "\n",
    "# Create output node g\n",
    "g = multiply(p,z)\n",
    "\n",
    "feed_dict = {\n",
    "    x: [1], y: [3], z: [-3]\n",
    "}\n",
    "\n",
    "# Create session\n",
    "session = Session()\n",
    "output = session.run(g, feed_dict)\n",
    "\n",
    "print(\"Forward Output: \",output)\n",
    "\n",
    "# Build backward_op\n",
    "backward_op = Backward().backward(g)\n",
    "output = session.run(backward_op, feed_dict)\n",
    "\n",
    "# Print final result\n",
    "print(\"Backward Output: \",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yrf-ew-ECOUj"
   },
   "source": [
    "**Example**\n",
    "Let's now test our implementation to compute the backward pass of computation graph for the following affine transformation:\n",
    "\\begin{equation}\n",
    "g = (x+2y)*z\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjqw8Ij3CP_V",
    "outputId": "509666b8-75b4-472d-e4e2-1a2dc75020fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forwrad: Nodes Postorder [<__main__.placeholder object at 0x7f963423e0d0>, <__main__.Variable object at 0x7f963423ed50>, <__main__.placeholder object at 0x7f963423e550>, <__main__.multiply object at 0x7f963423e690>, <__main__.add object at 0x7f963423eb90>, <__main__.placeholder object at 0x7f963423ebd0>, <__main__.multiply object at 0x7f963423e750>]\n",
      "Node Inputs [array([2]), array([3])]\n",
      "Node Inputs [array([1]), array([6])]\n",
      "Node Inputs [array([7]), array([-3])]\n",
      "Forward Output:  [-21]\n",
      "Forwrad: Nodes Postorder [<__main__.Backward.backward.<locals>.BackwardOperation object at 0x7f963423e590>]\n",
      "Node Inputs []\n",
      "compute_gradients\n",
      "\n",
      "Consumer:  <__main__.multiply object at 0x7f963423e750>\n",
      "grad wrt output:  1\n",
      "grad wrt inputs:  [array([-3]), array([7])]\n",
      "Grad Node:  [-3]\n",
      "\n",
      "Consumer:  <__main__.multiply object at 0x7f963423e750>\n",
      "grad wrt output:  1\n",
      "grad wrt inputs:  [array([-3]), array([7])]\n",
      "Grad Node:  [7]\n",
      "\n",
      "Consumer:  <__main__.add object at 0x7f963423eb90>\n",
      "grad wrt output:  [-3]\n",
      "grad wrt inputs:  [array([-3]), array([-3])]\n",
      "Grad Node:  [-3]\n",
      "\n",
      "Consumer:  <__main__.add object at 0x7f963423eb90>\n",
      "grad wrt output:  [-3]\n",
      "grad wrt inputs:  [array([-3]), array([-3])]\n",
      "Grad Node:  [-3]\n",
      "\n",
      "Consumer:  <__main__.multiply object at 0x7f963423e690>\n",
      "grad wrt output:  [-3]\n",
      "grad wrt inputs:  [array([-9]), array([-6])]\n",
      "Grad Node:  [-9]\n",
      "\n",
      "Consumer:  <__main__.multiply object at 0x7f963423e690>\n",
      "grad wrt output:  [-3]\n",
      "grad wrt inputs:  [array([-9]), array([-6])]\n",
      "Grad Node:  [-6]\n",
      "Backward Output:  None\n"
     ]
    }
   ],
   "source": [
    "# Create a new graph\n",
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "# Create variable\n",
    "two = Variable([2])\n",
    "\n",
    "# Create placeholder\n",
    "x = placeholder()\n",
    "y = placeholder()\n",
    "z = placeholder()\n",
    "\n",
    "# Create hidden node twoy\n",
    "twoy = multiply(two, y)\n",
    "\n",
    "# Create hidden node p\n",
    "p = add(x, twoy)\n",
    "\n",
    "# Create output node g\n",
    "g = multiply(p,z)\n",
    "\n",
    "feed_dict = {\n",
    "    x: [1], y: [3], z: [-3]\n",
    "}\n",
    "\n",
    "# Create session\n",
    "session = Session()\n",
    "output = session.run(g, feed_dict)\n",
    "\n",
    "print(\"Forward Output: \",output)\n",
    "\n",
    "# Build backward_op\n",
    "backward_op = Backward().backward(g)\n",
    "output = session.run(backward_op, feed_dict)\n",
    "\n",
    "# Print final result\n",
    "print(\"Backward Output: \",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uKZRQCz8cp7"
   },
   "source": [
    "**Example**\n",
    "Let's now test our implementation to compute the backward pass of computation graph for the following affine transformation:\n",
    "\\begin{equation}\n",
    "g = (a+b)*(b-c)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OfirjE_C8iqY",
    "outputId": "00e22b85-d6f0-4160-d807-38705ebe7434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forwrad: Nodes Postorder [<__main__.placeholder object at 0x7f9635498750>, <__main__.placeholder object at 0x7f9635498590>, <__main__.add object at 0x7f9635498e90>, <__main__.placeholder object at 0x7f9635498590>, <__main__.placeholder object at 0x7f96354986d0>, <__main__.negative object at 0x7f9635498b10>, <__main__.add object at 0x7f9635498ed0>, <__main__.multiply object at 0x7f9635498650>]\n",
      "Node Inputs [array([5]), array([-2])]\n",
      "Node Inputs [array([4])]\n",
      "Node Inputs [array([-2]), array([-4])]\n",
      "Node Inputs [array([3]), array([-6])]\n",
      "Forward Output:  [-18]\n",
      "Forwrad: Nodes Postorder [<__main__.Backward.backward.<locals>.BackwardOperation object at 0x7f963423e8d0>]\n",
      "Node Inputs []\n",
      "compute_gradients\n",
      "\n",
      "Consumer:  <__main__.multiply object at 0x7f9635498650>\n",
      "grad wrt output:  1\n",
      "grad wrt inputs:  [array([-6]), array([3])]\n",
      "Grad Node:  [-6]\n",
      "\n",
      "Consumer:  <__main__.multiply object at 0x7f9635498650>\n",
      "grad wrt output:  1\n",
      "grad wrt inputs:  [array([-6]), array([3])]\n",
      "Grad Node:  [3]\n",
      "\n",
      "Consumer:  <__main__.add object at 0x7f9635498e90>\n",
      "grad wrt output:  [-6]\n",
      "grad wrt inputs:  [array([-6]), array([-6])]\n",
      "Grad Node:  [-6]\n",
      "\n",
      "Consumer:  <__main__.add object at 0x7f9635498e90>\n",
      "grad wrt output:  [-6]\n",
      "grad wrt inputs:  [array([-6]), array([-6])]\n",
      "Grad Node:  [-6]\n",
      "\n",
      "Consumer:  <__main__.add object at 0x7f9635498ed0>\n",
      "grad wrt output:  [3]\n",
      "grad wrt inputs:  [array([3]), array([3])]\n",
      "Grad Node:  [-3]\n",
      "\n",
      "Consumer:  <__main__.add object at 0x7f9635498ed0>\n",
      "grad wrt output:  [3]\n",
      "grad wrt inputs:  [array([3]), array([3])]\n",
      "Grad Node:  [3]\n",
      "\n",
      "Consumer:  <__main__.negative object at 0x7f9635498b10>\n",
      "grad wrt output:  [3]\n",
      "grad wrt inputs:  [-3]\n",
      "Grad Node:  [-3]\n",
      "Backward Output:  None\n"
     ]
    }
   ],
   "source": [
    "# Create a new graph\n",
    "# Create a new graph\n",
    "Graph().as_default()\n",
    "\n",
    "# Create placeholder\n",
    "a = placeholder()\n",
    "b = placeholder()\n",
    "c = placeholder()\n",
    "\n",
    "# Create hidden node d\n",
    "d = add(a, b)\n",
    "\n",
    "# Create hidden node e\n",
    "nc = negative(c)\n",
    "e = add(b, nc)\n",
    "\n",
    "# Create output node g\n",
    "g = multiply(d,e)\n",
    "\n",
    "feed_dict = {\n",
    "    a: [5], b: [-2], c: [4]\n",
    "}\n",
    "\n",
    "# Create session\n",
    "session = Session()\n",
    "output = session.run(g, feed_dict)\n",
    "\n",
    "print(\"Forward Output: \",output)\n",
    "\n",
    "# Build backward_op\n",
    "backward_op = Backward().backward(g)\n",
    "output = session.run(backward_op, feed_dict)\n",
    "\n",
    "# Print final result\n",
    "print(\"Backward Output: \",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6ggD850vJ1U"
   },
   "source": [
    "\n",
    "**Task**\n",
    "\n",
    "Now you have these equation,\n",
    "\n",
    "\\begin{equation}\n",
    "g = (x*y)*z*y\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g = ((y+10z)*z*y)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "g = (x+10y*50z))*z*y*((x*y)*z*y)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "You need to do forward and backward pass on the given equation using above implemented method and also varify the answer using manual implemenation. You can use any value of x,y,z for result calculation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTT4HbYtyvCW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PA_1_HW1_RollNumber .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
